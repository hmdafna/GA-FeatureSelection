{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-multilearn scipy pymoo matplotlib scikit-learn minepy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "o_IoGTQCdEvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from scipy.sparse import lil_matrix\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.config import Config\n",
        "Config.warnings['not_compiled'] = False\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from pymoo.util.nds.non_dominated_sorting import NonDominatedSorting\n",
        "from minepy import pstats, cstats\n",
        "from pymoo.optimize import minimize\n",
        "from multiprocessing import Pool\n",
        "import pickle\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "6hIaEbBIdFUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_zero_columns(X):\n",
        "    # Find the columns that are all zeros\n",
        "    zero_columns = X.columns[(X == 0).all()]\n",
        "    # Get the indices of these columns\n",
        "    zero_column_indices = [X.columns.get_loc(col) for col in zero_columns]\n",
        "    return zero_column_indices\n",
        "\n",
        "\n",
        "def determine_ranks(all_Data):\n",
        "\n",
        "  sorted_data = []\n",
        "\n",
        "  for i in range(len(all_Data)):\n",
        "    name, data_matrix, feature_names, labels = all_Data[i]\n",
        "\n",
        "    print(\"The name of the data set is: \", name)\n",
        "    print(\"Feature names:\", feature_names)\n",
        "    print(\"Categories:\", labels)\n",
        "    print(\"Data matrix shape:\", data_matrix.shape)\n",
        "\n",
        "    # Convert the data matrix into a DataFrame\n",
        "    df = pd.DataFrame(data_matrix, columns=feature_names + labels)\n",
        "\n",
        "    # Separate the labels and features in the DataFrame\n",
        "    y = df[labels]\n",
        "    X = df.drop(columns=labels)\n",
        "\n",
        "    xtrain_validate, xtest, ytrain_validate, ytest = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
        "    xtrain, xvalidate, ytrain, yvalidate = train_test_split(xtrain_validate, ytrain_validate, test_size=0.2, random_state = 42)\n",
        "\n",
        "    zero_column_indices = find_all_zero_columns(xtrain_validate)\n",
        "    print(\"Indices of columns that are all zeros:\", zero_column_indices)\n",
        "    print(f\"There are {len(zero_column_indices)} features with no presence in the train validate sets\")\n",
        "    print(f\"The shape of this validate data matrix is: {xtrain_validate.shape}\")\n",
        "\n",
        "    ytrain_lil = lil_matrix(ytrain)\n",
        "    yvalidate_lil = lil_matrix(yvalidate)\n",
        "    ytest_lil = lil_matrix(ytest)\n",
        "\n",
        "    # calculate the mic matrix to select initial features for phase 1\n",
        "    xtrain_validate_transpose = xtrain_validate.transpose()\n",
        "    ytrain_validate_transpose = ytrain_validate.transpose()\n",
        "\n",
        "    mic_matrix, tic_c =  cstats(xtrain_validate_transpose.values, ytrain_validate_transpose.values, alpha=9, c=5, est=\"mic_e\")\n",
        "\n",
        "    mic_matrix = np.array(mic_matrix)\n",
        "\n",
        "    # Instantiate the NonDominatedSorting class\n",
        "    nds = NonDominatedSorting(maximize=True)\n",
        "\n",
        "    # Perform non-dominated sorting on the mic_matrix\n",
        "    sorted_indices = nds.do(mic_matrix, only_non_dominated_front=False)\n",
        "\n",
        "    for i in range(len(sorted_indices)):\n",
        "      print(f'Rank {i} in {name} has {len(sorted_indices[i])} features: ', sorted_indices[i])\n",
        "\n",
        "    print(f\"The are {xtrain.shape[1]} features total\")\n",
        "\n",
        "    print(\"\\n#######################################################################################\\n\")\n",
        "\n",
        "    sorted_data.append((name, sorted_indices))\n",
        "\n",
        "  return sorted_data"
      ],
      "metadata": {
        "id": "8UfQMWAVysfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "all_Data = []\n",
        "\n",
        "data_matrix_name, feature_names_name, labels_name = parse_arff_data_name('name.arff')\n",
        "name = ['name', data_matrix_name, feature_names_name, labels_name]\n",
        "all_Data.append(name)"
      ],
      "metadata": {
        "id": "W75kbc8Pdjle",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(dataset):\n",
        "    # Extract dataset information\n",
        "    name, data_matrix, feature_names, labels = dataset\n",
        "\n",
        "    # Print dataset being processed\n",
        "    print(f\"Processing dataset: {name}\")\n",
        "\n",
        "    # Call determine_ranks function for the current dataset\n",
        "    sorted_data = determine_ranks([dataset])\n",
        "\n",
        "    return (name, sorted_data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sorted_datasets = []\n",
        "\n",
        "    # Process each dataset sequentially\n",
        "    for dataset in all_Data:\n",
        "        sorted_dataset = process_dataset(dataset)\n",
        "        sorted_datasets.append(sorted_dataset)\n",
        "\n",
        "    # Save sorted_data to a pickle file\n",
        "    with open('sorted_data.pkl', 'wb') as f:\n",
        "        pickle.dump(sorted_datasets, f)\n",
        "\n",
        "    # Download the saved pickle file (Google Colab specific)\n",
        "    files.download('sorted_data.pkl')"
      ],
      "metadata": {
        "id": "wrg5dHtdzLR2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}