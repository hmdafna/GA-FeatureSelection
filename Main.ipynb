{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the core components of the algorithm. It trains the MLKNN model using features selected by a genetic algorithm over 100 generations (modifiable as needed). The genetic algorithm is applied to both the filtered and original sets of features, aiming to minimize the number of features and the hamming loss. The objective values are then saved into a pickle file witht he oprion to save all feature populations for future evaluations and inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-multilearn scipy pymoo matplotlib scikit-learn minepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.metrics import hamming_loss\n",
    "from pymoo.core.problem import Problem\n",
    "import time\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.operators.sampling.rnd import BinaryRandomSampling\n",
    "from pymoo.operators.crossover.ux import UniformCrossover\n",
    "from pymoo.operators.mutation.bitflip import BitflipMutation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pymoo.indicators.hv import HV\n",
    "from pymoo.config import Config\n",
    "Config.warnings['not_compiled'] = False\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from pymoo.util.nds.non_dominated_sorting import NonDominatedSorting\n",
    "from minepy import pstats, cstats\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from random import randrange\n",
    "from pymoo.core.sampling import Sampling\n",
    "from pymoo.core.duplicate import ElementwiseDuplicateElimination\n",
    "import pickle\n",
    "from pymoo.core.crossover import Crossover\n",
    "import os\n",
    "from pymoo.core.mutation import Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code extracts the information obtained in the initial filteration, saved in the 'sorted_data.pkl' file. Only the indices of the features in the top front (ie. rank 0) are added to the 'tops' array. This might need modifications depending on how the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pickle file in read-binary mode and load the data\n",
    "with open('sorted_data.pkl', 'rb') as f:\n",
    "    sorted_data = pickle.load(f)\n",
    "\n",
    "tops = []\n",
    "# Print the loaded data to verify\n",
    "for name, sorted_indices in sorted_data:\n",
    "    print(f\"Dataset name: {name}\")\n",
    "    tops.append(sorted_indices[0])\n",
    "\n",
    "print(tops[0]) # replace witht he index of the current datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MyProblem class defines a multi-objective optimization problem for feature selection in a multi-label classification task using the MLkNN algorithm. It evaluates different feature subsets based on the number of selected features and the Hamming loss on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProblem(Problem):\n",
    "  def __init__(self, xtrain, ytrain_lil, xvalidate, yvalidate_lil):\n",
    "      super().__init__(n_var=xtrain.shape[1], n_obj=2, n_ieq_constr=1, xl=np.zeros(xtrain.shape[1]), xu=np.ones(xtrain.shape[1]))\n",
    "      self.xtrain = xtrain\n",
    "      self.ytrain_lil = ytrain_lil\n",
    "      self.xvalidate = xvalidate\n",
    "      self.yvalidate_lil = yvalidate_lil\n",
    "      self.feature_names = xtrain.columns.tolist()\n",
    "      self.total_features = xtrain.shape[1]  # Store the total number of features\n",
    "\n",
    "  def _evaluate(self, x, out, *args, **kwargs):\n",
    "      f1 = []\n",
    "      f2 = []\n",
    "\n",
    "      for bitstring in x:\n",
    "          featureNames = [self.feature_names[i] for i, bit in enumerate(bitstring) if bit == 1]\n",
    "          if not featureNames:\n",
    "              ham_loss = 1  # High hamming loss if no features are selected\n",
    "              num_features = 0\n",
    "          else:\n",
    "              trainx = self.xtrain[featureNames]\n",
    "              classifier = MLkNN(k=3)\n",
    "              classifier.fit(trainx, self.ytrain_lil)\n",
    "\n",
    "              validatex = self.xvalidate[featureNames]\n",
    "\n",
    "              predy = classifier.predict(validatex)\n",
    "              ham_loss = hamming_loss(self.yvalidate_lil.toarray(), predy.toarray())\n",
    "              num_features = len(featureNames)\n",
    "\n",
    "          # Calculate the ratio of selected features to total features\n",
    "          feature_ratio = num_features / self.total_features\n",
    "\n",
    "          f1.append(feature_ratio)\n",
    "          f2.append(ham_loss)  # Hamming loss is already a value between 0 and 1\n",
    "\n",
    "      # Constraint: At least one feature must be selected\n",
    "      g1 = [1 - sum(bitstring) for bitstring in x]  # Constraint that at least one feature must be selected\n",
    "\n",
    "      out[\"F\"] = np.column_stack((f1, f2))\n",
    "      out[\"G\"] = np.column_stack(g1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm defines the NSGA-II optimization algorithm for solving the multi-objective feature selection problem. It initializes a population, performs binary uniform crossover and bit-flip mutation, and eliminates duplicate solutions to evolve optimal feature subsets.\n",
    "\n",
    "BinaryRandomSampling Class:\n",
    "The BinaryRandomSampling class is designed to generate an initial population for a binary optimization problem. Each individual in the population is represented by a binary string, where each bit indicates the presence (1) or absence (0) of a feature. This class ensures that each individual in the population is unique.\n",
    "\n",
    "BinaryUniformCrossover Class:\n",
    "The BinaryUniformCrossover class implements a uniform crossover operator for binary genetic algorithms. It creates offspring by combining features from two parent individuals based on a specified crossover probability.\n",
    "\n",
    "BinaryBitflipMutation Class:\n",
    "The BinaryBitflipMutation class implements a bit-flip mutation operator for binary genetic algorithms. It mutates individuals by flipping each bit with a specified probability.\n",
    "\n",
    "create_nsga2_algorithm Function:\n",
    "The create_nsga2_algorithm function creates an instance of the NSGA2 algorithm for multi-objective optimization. It uses the previously defined BinaryRandomSampling, BinaryUniformCrossover, and BinaryBitflipMutation classes to define the algorithm's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryRandomSampling(Sampling):\n",
    "    def __init__(self, num_features, pop_size=100):\n",
    "        self.num_features = num_features\n",
    "        self.pop_size = pop_size\n",
    "        super().__init__()\n",
    "\n",
    "    def _do(self, problem, n_samples, **kwargs):\n",
    "        population = []\n",
    "        seen = set()\n",
    "        feature_counts = []\n",
    "        population.append(np.ones(self.num_features, dtype=int))\n",
    "        while len(population) < self.pop_size: # keep regenerating unique individuals\n",
    "            rn = randrange(0, self.num_features + 1, 1)  # +1 to include all features\n",
    "            arr = np.zeros(self.num_features, dtype=int)\n",
    "            arr[:rn] = 1\n",
    "            np.random.shuffle(arr)\n",
    "            arr_tuple = tuple(arr)\n",
    "            if arr_tuple not in seen:\n",
    "                population.append(arr)\n",
    "                seen.add(arr_tuple)\n",
    "                feature_counts.append(rn)\n",
    "            else:\n",
    "                print(\"Duplicate found during initialization and ignored:\", arr_tuple)\n",
    "\n",
    "        # Plotting to verify the distribution is uniform\n",
    "        plt.figure(figsize=(6, 4))  # Adjust the figure size as needed\n",
    "        plt.hist(feature_counts, bins='auto', color='blue', alpha=0.7, rwidth=0.75)\n",
    "        plt.grid(axis='y', alpha=0.75)\n",
    "        plt.xlabel('Number of Features', fontsize=9)  # Adjust font size\n",
    "        plt.ylabel('Frequency', fontsize=9)  # Adjust font size\n",
    "        plt.title('Distribution of Number of Features (Verification of Initialization)', fontsize=11)  # Adjust font size\n",
    "        plt.savefig(\"Histogram_of_Initial_Population.eps\", format='eps')\n",
    "        plt.show()\n",
    "        print()\n",
    "        return np.array(population)\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class BinaryUniformCrossover(Crossover):\n",
    "\n",
    "    def __init__(self, prob=0.5):\n",
    "        super().__init__(2, 2)\n",
    "        self.prob = prob\n",
    "\n",
    "    def _do(self, _, X, **kwargs):\n",
    "        # print(\"using the manually defined crossover\")\n",
    "        _, n_matings, n_var = X.shape\n",
    "        M = np.random.random((n_matings, n_var)) < self.prob\n",
    "        offspring = np.zeros_like(X)\n",
    "        for i in range(n_matings):\n",
    "            for j in range(n_var):\n",
    "                if M[i, j]:\n",
    "                    offspring[0, i, j] = X[1, i, j]\n",
    "                    offspring[1, i, j] = X[0, i, j]\n",
    "                else:\n",
    "                    offspring[0, i, j] = X[0, i, j]\n",
    "                    offspring[1, i, j] = X[1, i, j]\n",
    "        return offspring\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class BinaryBitflipMutation(Mutation):\n",
    "    def __init__(self, prob=0.01, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.prob = prob\n",
    "\n",
    "    def _do(self, problem, X, **kwargs):\n",
    "        # print(\"using the manually defined mutation\")\n",
    "        prob_var = self.get_prob_var(problem, size=(len(X), 1))\n",
    "        Xp = np.copy(X)\n",
    "        flip = np.random.random(X.shape) < self.prob * prob_var\n",
    "        Xp[flip] = ~X[flip]\n",
    "        return Xp\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "def create_nsga2_algorithm(total_features):\n",
    "    return NSGA2(\n",
    "        pop_size=100,\n",
    "        sampling=BinaryRandomSampling(total_features),\n",
    "        crossover=BinaryUniformCrossover(prob=0.9),\n",
    "        mutation=BinaryBitflipMutation(prob=0.01),\n",
    "        eliminate_duplicates=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot_final_pareto_front function visualizes the final Pareto front obtained from the NSGA2 optimization process. This plot helps in understanding the trade-off between the number of selected features and the validation error (Hamming loss) for the multi-label classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_pareto_front(pop, f, total_features, type, dataset, run_num): # this is the output of nsga2 directly (ie. the validation)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    actual_features_pop = pop.get(\"F\")[:,0] * total_features\n",
    "    actual_features_f = f[:,0] * total_features\n",
    "    plt.scatter(actual_features_pop, pop.get(\"F\")[:,1], edgecolor=\"blue\", facecolor=\"none\", label=\"Solutions\")\n",
    "    plt.scatter(actual_features_f, f[:,1], marker='*', edgecolor=\"red\", facecolor=\"none\", label=\"Non-dominated Pareto Front\")\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Validation Error (Hamming Loss)')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Final Pareto Front of the {dataset} dataset with {type} data on validation\")\n",
    "    \n",
    "    # Save the figure\n",
    "    filename = f\"Final_Pareto_Front_with_{type}_data_on_validation_run {run_num}.eps\"\n",
    "    plt.savefig(filename, format='eps')\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculate_metrics function computes several performance metrics for the Pareto front obtained from the NSGA2 optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(res, xtrain, ytrain_lil, xvalidate, yvalidate_lil, xtest, ytest_lil, total_features, type, dataset, run_num):\n",
    "    pop = res.pop\n",
    "    F = res.F\n",
    "\n",
    "    # Find the solution with the minimum Hamming loss on the validation set, F[:, 1] holds the values of the hamming losses\n",
    "    min_hl_val_idx = np.argmin(F[:, 1])\n",
    "\n",
    "    # extracting the minimum hamming loss value as well as the numebr of features used to obtain this value\n",
    "    min_HL_val = F[min_hl_val_idx, 1]\n",
    "    min_f_val = F[min_hl_val_idx, 0] * total_features # adjust to include the number of features not the scaling\n",
    "\n",
    "    # Calculate the hamming loss of the final pareto front on the test set\n",
    "    num_features = []\n",
    "    hl_test = []\n",
    "    for i in range(len(pop)):\n",
    "        selected_features = np.where(pop[i].get(\"X\") == 1)[0]\n",
    "        if len(selected_features) == 0:\n",
    "            ham_loss = 1.0  # If no features selected, set a high hamming loss\n",
    "        else:\n",
    "            model = MLkNN(k=3)\n",
    "            model.fit(xtrain.iloc[:, selected_features], ytrain_lil)\n",
    "            predy = model.predict(xtest.iloc[:, selected_features])\n",
    "            ham_loss = hamming_loss(ytest_lil.toarray(), predy.toarray())\n",
    "\n",
    "        num_features.append(len(selected_features))\n",
    "        hl_test.append(ham_loss)\n",
    "\n",
    "    # Convert test results to numpy array for plotting\n",
    "    test_F = np.column_stack((num_features, hl_test))\n",
    "\n",
    "    # Non-dominated sorting to find the Pareto-optimal solutions in the test set\n",
    "    nds = NonDominatedSorting()\n",
    "    pareto_optimal_indices_test = nds.do(test_F, only_non_dominated_front=True)\n",
    "    pareto_optimal_test_F = test_F[pareto_optimal_indices_test]\n",
    "\n",
    "    # Scale the objective values using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    pareto_optimal_test_F_scaled = scaler.fit_transform(pareto_optimal_test_F)\n",
    "\n",
    "    # Define the reference point\n",
    "    ref_point = np.array([1.1, 1.1]) # if we have (1,1) will arise a probelm if the paretofront has only 1 solution at the point (1,1)\n",
    "\n",
    "    # Calculate hypervolume for Pareto-optimal solutions in the test set\n",
    "    hv_indicator_test = HV(ref_point=ref_point)\n",
    "    HV_test = hv_indicator_test(pareto_optimal_test_F_scaled)\n",
    "\n",
    "    # Calculate hypervolume for Pareto-optimal solutions in the validation set\n",
    "    pareto_optimal_val_F = F  # The F already contains the Pareto-optimal front\n",
    "    ref_point_val = np.array([1.1, 1.1])  # The referenc epoint shouldnt be 1,1, 1.1 is ebtter\n",
    "    pareto_optimal_val_F_scaled = scaler.fit_transform(pareto_optimal_val_F) \n",
    "    hv_indicator_val = HV(ref_point=ref_point_val)\n",
    "    HV_val = hv_indicator_val(pareto_optimal_val_F_scaled)\n",
    "\n",
    "    # Find the solution with the minimum Hamming loss on the test set\n",
    "    min_HL_test_idx = np.argmin(pareto_optimal_test_F[:, 1])\n",
    "    min_HL_test = pareto_optimal_test_F[min_HL_test_idx, 1]\n",
    "    min_f_test = pareto_optimal_test_F[min_HL_test_idx, 0]\n",
    "\n",
    "    # Plot Hamming loss values for test set of solutions in optimal Pareto front\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(test_F[:, 0], test_F[:, 1], edgecolor=\"blue\", facecolor=\"none\", label=\"Solutions\")\n",
    "    plt.scatter(pareto_optimal_test_F[:, 0], pareto_optimal_test_F[:, 1], marker='*', edgecolor=\"red\", facecolor=\"none\", label=\"Non Dominated Pareto Front\")\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Hamming Loss (Test)')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Final Pareto Front of the {dataset} dataset with {type} data on test\")\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Save the figure\n",
    "    filename = f\"Final_Pareto_Front_with_{type}_data_on_test_run {run_num}.eps\"\n",
    "    plt.savefig(filename, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return  min_HL_val, min_f_val, min_HL_test, min_f_test, HV_val, HV_test, hl_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, load the datasets from the ARFF files. The dataframe is then split into training, validation, and test sets, allocating 50%, 20%, and 30% of the data, respectively. The labels and features are separated accordingly. The labels are converted into a lil_matrix format for compatibility with the MLkNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix, feature_names, labels = parse_arff_data_name('name.arff') # replace with dataset name and reading function\n",
    "\n",
    "# Convert the data matrix into a DataFrame\n",
    "df = pd.DataFrame(data_matrix, columns=feature_names + labels)\n",
    "\n",
    "# Separate the labels and features in the DataFrame\n",
    "y = df[labels]\n",
    "X = df.drop(columns=labels)\n",
    "\n",
    "print(f\"The shape of the data matrix is {data_matrix.shape} with {len(feature_names)} features and {len(labels)} labels. \\n\")\n",
    "\n",
    "xtrain_validate, xtest, ytrain_validate, ytest = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "xtrain, xvalidate, ytrain, yvalidate = train_test_split(xtrain_validate, ytrain_validate, test_size=0.2, random_state = 42)\n",
    "\n",
    "ytrain_lil = lil_matrix(ytrain)\n",
    "yvalidate_lil = lil_matrix(yvalidate)\n",
    "ytest_lil = lil_matrix(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifer is train with k = 3 and hamming loss values are obtained for both the validation and test tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_all_features = MLkNN(k=3)\n",
    "classifier_all_features.fit(xtrain, ytrain_lil)\n",
    "\n",
    "# Predict labels for the validation set using all features\n",
    "predy_all_features_validate = classifier_all_features.predict(xvalidate)\n",
    "\n",
    "# Predict labels for the test set using all features\n",
    "predy_all_features_test = classifier_all_features.predict(xtest)\n",
    "\n",
    "# Calculate Hamming loss for validation set when all features are used\n",
    "ham_loss_all_features_validate = hamming_loss(yvalidate_lil.toarray(), predy_all_features_validate.toarray())\n",
    "\n",
    "# Calculate Hamming loss for test set when all features are used\n",
    "ham_loss_all_features_test = hamming_loss(ytest_lil.toarray(), predy_all_features_test.toarray())\n",
    "\n",
    "print(f\"The Hamming loss resulting on the validation set using all {xtrain.shape[1]} features is {ham_loss_all_features_validate}\")\n",
    "print(f\"The Hamming loss resulting on the test set using all {xtrain.shape[1]} features is {ham_loss_all_features_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing code section filters the dataset to include only features obtained from the initial filteration technique. Then, the classifer is train with k = 3 and hamming loss values are obtained for both the validation and test tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_front = tops[0] # replace with the index of the current dataset\n",
    "\n",
    "print(f\"There are {len(top_front)} features in the top front which are being used in the second phase.\")\n",
    "\n",
    "# Extract the selected features from xtrain_validate using iloc\n",
    "selected_features_phase1 = xtrain_validate.iloc[:, top_front]\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names_phase1 = selected_features_phase1.columns.tolist()\n",
    "\n",
    "# Select the features from the training, validation, and test sets\n",
    "xtrain_filtered = xtrain.iloc[:, top_front]\n",
    "xvalidate_filtered = xvalidate.iloc[:, top_front]\n",
    "xtest_filtered = xtest.iloc[:, top_front]\n",
    "\n",
    "# Instantiate the MLkNN classifier with k=3\n",
    "model = MLkNN(k=3)\n",
    "\n",
    "# Fit the model using the selected features from the training data\n",
    "model.fit(xtrain_filtered, ytrain_lil)\n",
    "\n",
    "# Predict labels for the validation data using the trained model and selected features\n",
    "val_predy = model.predict(xvalidate_filtered)\n",
    "\n",
    "# Predict labels for the test data using the trained model and selected features\n",
    "test_predy = model.predict(xtest_filtered)\n",
    "\n",
    "# Calculate the Hamming loss between the true labels and predicted labels for the validation data\n",
    "ham_loss_all_features_from_phase_1_validate = hamming_loss(yvalidate_lil.toarray(), val_predy.toarray())\n",
    "\n",
    "# Calculate the Hamming loss between the true labels and predicted labels for the test data\n",
    "ham_loss_all_features_from_phase_1_test = hamming_loss(ytest_lil.toarray(), test_predy.toarray())\n",
    "\n",
    "print(f\"The Hamming loss resulting on the validation set using the {xtrain_filtered.shape[1]} filtered features is {ham_loss_all_features_from_phase_1_validate}.\")\n",
    "print(f\"The Hamming loss resulting on the test set using the {xtrain_filtered.shape[1]} filtered features is {ham_loss_all_features_from_phase_1_test}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roblem_not_filtered = MyProblem(xtrain, ytrain_lil, xvalidate, yvalidate_lil)\n",
    "problem_filtered = MyProblem(xtrain_filtered, ytrain_lil, xvalidate_filtered, yvalidate_lil)\n",
    "\n",
    "def run_optimization(problem, algorithm, n_gen):\n",
    "    res = minimize(problem, algorithm, ('n_gen',n_gen), verbose=True, save_history=False)\n",
    "    return res\n",
    "\n",
    "pickle_files = []\n",
    "\n",
    "# Determine the number of runs you want to obtain.\n",
    "for j in range(5):\n",
    "    print(f\"Running the algorithm for hte {j} time. \\n\")\n",
    "\n",
    "    res_not_filtered = run_optimization(problem_not_filtered, create_nsga2_algorithm(xtrain.shape[1]), 100) # 100 defines the number of generations the algorithm will run for.\n",
    "    res_filtered = run_optimization(problem_filtered, create_nsga2_algorithm(xtrain_filtered.shape[1]), 100)\n",
    "\n",
    "    # Plot final Pareto front for both res\n",
    "    non_dominated_feature_subsets_not_filtered = plot_final_pareto_front(res_not_filtered.pop, res_not_filtered.F, xtrain.shape[1], 'unfiltered', 'name', j)\n",
    "    non_dominated_feature_subsets_filtered = plot_final_pareto_front(res_filtered.pop, res_filtered.F, xtrain_filtered.shape[1], 'filtered', 'name', j)\n",
    "\n",
    "    # Calculate metrics for unfiltered data\n",
    "    min_HL_val_nf, min_f_val_nf, min_HL_test_nf, min_f_test_nf, HV_val_nf, HV_test_nf, hamming_losses_on_test_unfilitered = calculate_metrics(res_not_filtered, xtrain, ytrain_lil, xvalidate, yvalidate_lil, xtest, ytest_lil, xtrain.shape[1], 'unfiltered', 'name', j)\n",
    "\n",
    "    # Calculate metrics for filtered data\n",
    "    min_HL_val_f, min_f_val_f, min_HL_test_f, min_f_test_f, HV_val_f, HV_test_f,  hamming_losses_on_test_filtered = calculate_metrics(res_filtered, xtrain_filtered, ytrain_lil, xvalidate_filtered, yvalidate_lil, xtest_filtered, ytest_lil, xtrain_filtered.shape[1], 'filtered', 'name', j)\n",
    "\n",
    "    pop_not_filtered = res_not_filtered.pop\n",
    "    pop_filtered = res_filtered.pop\n",
    "\n",
    "    f_not_filtered = res_not_filtered.F\n",
    "    f_filtered = res_filtered.F\n",
    "\n",
    "    hamming_loss_on_validation_not_filtered = pop_not_filtered.get(\"F\")[:,1]\n",
    "    hamming_loss_on_validation_filtered = pop_filtered.get(\"F\")[:,1]\n",
    "\n",
    "    total_features_not_filtered = xtrain.shape[1]\n",
    "    total_features_filtered = xtrain_filtered.shape[1]\n",
    "\n",
    "    ###############################################################################################\n",
    "    # Save the data in a pickle file\n",
    "\n",
    "    # Define your results dictionary with simpler keys\n",
    "    results = {\n",
    "        \"Run number :\": j,\n",
    "        \"HamLoss_Val_All\": ham_loss_all_features_validate,\n",
    "        \"HamLoss_Test_All\": ham_loss_all_features_test,\n",
    "        \"HamLoss_Val_Filtered\": ham_loss_all_features_from_phase_1_validate,\n",
    "        \"HamLoss_Test_Filtered\": ham_loss_all_features_from_phase_1_test,\n",
    "        \"MinHamLoss_Val_Unf\": min_HL_val_nf,\n",
    "        \"NumFeatures_MinHamLoss_Val_Unf\": min_f_val_nf,\n",
    "        \"MinHamLoss_Test_Unf\": min_HL_test_nf,\n",
    "        \"NumFeatures_MinHamLoss_Test_Unf\": min_f_test_nf,\n",
    "        \"MinHamLoss_Val_Filt\": min_HL_val_f,\n",
    "        \"NumFeatures_MinHamLoss_Val_Filt\": min_f_val_f,\n",
    "        \"MinHamLoss_Test_Filt\": min_HL_test_f,\n",
    "        \"NumFeatures_MinHamLoss_Test_Filt\": min_f_test_f,\n",
    "        \"HV_Val_Unf\": HV_val_nf,\n",
    "        \"HV_Test_Unf\": HV_test_nf,\n",
    "        \"HV_Val_Filt\": HV_val_f,\n",
    "        \"HV_Test_Filt\": HV_test_f\n",
    "    }\n",
    "\n",
    "    # Create a dictionary to store the combined results\n",
    "    combined_results = {}\n",
    "\n",
    "    # Copy the existing results to the combined results\n",
    "    combined_results.update(results)\n",
    "\n",
    "    # Add information for each individual in the unfiltered population\n",
    "    for i, individual in enumerate(pop_not_filtered):\n",
    "        x = individual.get(\"X\")  # Decision variables (selected features)\n",
    "        f = individual.get(\"F\")  # Objective values (number of features ratio, validation error)\n",
    "\n",
    "        # Multiply the first objective by the total number of features\n",
    "        num_features = f[0] * total_features_not_filtered\n",
    "        validation_error = f[1]\n",
    "\n",
    "        # Get the corresponding Hamming loss on the test set\n",
    "        test_error = hamming_losses_on_test_unfilitered[i]\n",
    "\n",
    "        # Create a dictionary for each individual\n",
    "        individual_dict = {\n",
    "            \"Selected Features\": x,\n",
    "            \"Number of Features\": num_features,\n",
    "            \"Hamming Loss on Validation Set\": validation_error,\n",
    "            \"Hamming Loss on Test Set\": test_error\n",
    "        }\n",
    "\n",
    "        # Add the individual's information to the combined results\n",
    "        combined_results[f\"Individual {i + 1} (Unfiltered Population)\"] = individual_dict\n",
    "\n",
    "    # Add information for each individual in the filtered population\n",
    "    for i, individual in enumerate(pop_filtered):\n",
    "        x = individual.get(\"X\")  # Decision variables (selected features)\n",
    "        f = individual.get(\"F\")  # Objective values (number of features ratio, validation error)\n",
    "\n",
    "        # Multiply the first objective by the total number of features\n",
    "        num_features = f[0] * total_features_filtered\n",
    "        validation_error = f[1]\n",
    "\n",
    "        # Get the corresponding Hamming loss on the test set\n",
    "        test_error = hamming_losses_on_test_filtered[i]\n",
    "\n",
    "        # Create a dictionary for each individual\n",
    "        individual_dict = {\n",
    "            \"Selected Features\": x,\n",
    "            \"Number of Features\": num_features,\n",
    "            \"Hamming Loss on Validation Set\": validation_error,\n",
    "            \"Hamming Loss on Test Set\": test_error\n",
    "        }\n",
    "\n",
    "        # Add the individual's information to the combined results\n",
    "        combined_results[f\"Individual {i + 1} (Filtered Population)\"] = individual_dict\n",
    "\n",
    "    # Save the combined results to a file (e.g., pickle)\n",
    "    pickle_file = f\"combined_results{j}.pkl\"\n",
    "\n",
    "    with open(f\"combined_results{j}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(combined_results, f)\n",
    "    print(\"Combined results saved to combined_results.pkl\")\n",
    "\n",
    "    pickle_files.append(pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
