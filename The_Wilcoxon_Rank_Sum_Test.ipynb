{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file performs statistical tests using the Wilcoxon rank-sum test to determine whether differences in metric values (objectives) between filtered and unfiltered datasets are statistically significant. It also includes functions to calculate averages, plot relationships between features and metrics, and provide detailed output on the performance of each metric. The function expects the input in the form of a cvs file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install matplotlib pandas scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t1-f9SLNeF2X"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Statistical test on each metric separately\n",
        "alpha = 0.05 (5%)\n",
        "if p < 0.05, reject H0, else accept H0\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ranksums\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "def perform_wilcoxon_test(group1, group2, metric_name, is_hamming_loss=False):\n",
        "    stat, p = ranksums(group1, group2)\n",
        "    median_group1 = np.median(group1)\n",
        "    median_group2 = np.median(group2)\n",
        "\n",
        "    print(f\"Test for {metric_name}\")\n",
        "    print(f\"Statistic: {stat}, p-value: {p}\")\n",
        "    if p < alpha:\n",
        "        print(\"Reject the null hypothesis: There is a significant difference between filtered and unfiltered data.\")\n",
        "        if is_hamming_loss:\n",
        "            if median_group1 < median_group2:\n",
        "                print(f\"Unfiltered data performs better for {metric_name}.\")\n",
        "            else:\n",
        "                print(f\"Filtered data performs better for {metric_name}.\")\n",
        "        else:\n",
        "            if median_group1 < median_group2:\n",
        "                print(f\"Filtered data performs better for {metric_name}.\")\n",
        "            else:\n",
        "                print(f\"Unfiltered data performs better for {metric_name}.\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis: There is no significant difference between filtered and unfiltered data.\")\n",
        "        if is_hamming_loss:\n",
        "            if median_group1 < median_group2:\n",
        "                print(f\"Unfiltered data tends to perform better for {metric_name}, but not significantly.\")\n",
        "            else:\n",
        "                print(f\"Filtered data tends to perform better for {metric_name}, but not significantly.\")\n",
        "        else:\n",
        "            if median_group1 < median_group2:\n",
        "                print(f\"Filtered data tends to perform better for {metric_name}, but not significantly.\")\n",
        "            else:\n",
        "                print(f\"Unfiltered data tends to perform better for {metric_name}, but not significantly.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Plotting function\n",
        "def plot_features_vs_hl(num_features, min_hl, title, color, label):\n",
        "    plt.scatter(num_features, min_hl, color=color, label=label)\n",
        "\n",
        "    if len(np.unique(num_features)) >= 2:\n",
        "        try:\n",
        "            plt.plot(np.unique(num_features), np.poly1d(np.polyfit(num_features, min_hl, 1))(np.unique(num_features)), color=color)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(f\"Warning: LinAlgError encountered while fitting polynomial for {label}. Plotting scatter only.\")\n",
        "    else:\n",
        "        print(f\"Warning: Not enough unique feature numbers to fit a polynomial for {label}. Plotting scatter only.\")\n",
        "\n",
        "    plt.xlabel('Number of Features')\n",
        "    plt.ylabel('Minimum Hamming Loss')\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "\n",
        "def print_with_average(run, arrays):\n",
        "    print(\"Run #:\", run, \"\\n\")\n",
        "    for name, array in arrays.items():\n",
        "        average = sum(array) / len(array) if len(array) > 0 else 0\n",
        "        print(f\"{name}: {array} \\nAverage {name}: {average}\\n\")\n",
        "\n",
        "def read_and_test(df, name):\n",
        "  # Strip any leading or trailing spaces from column names\n",
        "  df.columns = df.columns.str.strip()\n",
        "\n",
        "  # Extract the columns into arrays\n",
        "  run = df['Run #'].values\n",
        "  hamloss_val_all = df['HamLoss_Val_All'].values\n",
        "  hamloss_test_all = df['HamLoss_Test_All'].values\n",
        "  hamloss_val_filtered = df['HamLoss_Val_Filtered'].values\n",
        "  hamloss_test_filtered = df['HamLoss_Test_Filtered'].values\n",
        "  minhamloss_val_unf = df['MinHamLoss_Val_Unf'].values\n",
        "  numfeatures_minhamloss_val_unf = df['NumFeatures_MinHamLoss_Val_Unf'].values\n",
        "  minhamloss_test_unf = df['MinHamLoss_Test_Unf'].values\n",
        "  numfeatures_minhamloss_test_unf = df['NumFeatures_MinHamLoss_Test_Unf'].values\n",
        "  minhamloss_val_filt = df['MinHamLoss_Val_Filt'].values\n",
        "  numfeatures_minhamloss_val_filt = df['NumFeatures_MinHamLoss_Val_Filt'].values\n",
        "  minhamloss_test_filt = df['MinHamLoss_Test_Filt'].values\n",
        "  numfeatures_minhamloss_test_filt = df['NumFeatures_MinHamLoss_Test_Filt'].values\n",
        "  hv_val_unf = df['HV_Val_Unf'].values\n",
        "  hv_test_unf = df['HV_Test_Unf'].values\n",
        "  hv_val_filt = df['HV_Val_Filt'].values\n",
        "  hv_test_filt = df['HV_Test_Filt'].values\n",
        "\n",
        "  print(f'Dataset: {name}')\n",
        "\n",
        "  # Print the average values of each column\n",
        "  arrays = {\n",
        "    \"HamLoss_Val_All\": hamloss_val_all,\n",
        "    \"HamLoss_Val_Filtered\": hamloss_val_filtered,\n",
        "    \"MinHamLoss_Val_Unf\": minhamloss_val_unf,\n",
        "    \"NumFeatures_MinHamLoss_Val_Unf\": numfeatures_minhamloss_val_unf,\n",
        "    \"MinHamLoss_Val_Filt\": minhamloss_val_filt,\n",
        "    \"NumFeatures_MinHamLoss_Val_Filt\": numfeatures_minhamloss_val_filt,\n",
        "    \"HV_Val_Unf\": hv_val_unf,\n",
        "    \"HV_Val_Filt\": hv_val_filt,\n",
        "    \"HamLoss_Test_All\": hamloss_test_all,\n",
        "    \"HamLoss_Test_Filtered\": hamloss_test_filtered,\n",
        "    \"MinHamLoss_Test_Unf\": minhamloss_test_unf,\n",
        "    \"NumFeatures_MinHamLoss_Test_Unf\": numfeatures_minhamloss_test_unf,\n",
        "    \"MinHamLoss_Test_Filt\": minhamloss_test_filt,\n",
        "    \"NumFeatures_MinHamLoss_Test_Filt\": numfeatures_minhamloss_test_filt,\n",
        "    \"HV_Test_Unf\": hv_test_unf,\n",
        "    \"HV_Test_Filt\": hv_test_filt\n",
        "  }\n",
        "  print_with_average(run, arrays)\n",
        "\n",
        "  # Minimum Hamming Loss Tests\n",
        "  # Number of Features Tests\n",
        "  # Average of Minimum Hamming Loss Values\n",
        "  # Hypervolume Tests\n",
        "  # Average Hypervolume\n",
        "\n",
        "  perform_wilcoxon_test(minhamloss_val_unf, minhamloss_val_filt, \"Minimum Hamming Loss Values on Validation Set\", True)\n",
        "  perform_wilcoxon_test(numfeatures_minhamloss_val_unf, numfeatures_minhamloss_val_filt, \"Number of Features for Min Ham Loss on Validation Set\", True)\n",
        "  avg_min_HL_val_unf = np.mean(minhamloss_val_unf)\n",
        "  avg_min_HL_val_filt = np.mean(minhamloss_val_filt)\n",
        "  perform_wilcoxon_test(minhamloss_val_unf, minhamloss_val_filt, \"Average Minimum Hamming Loss on Validation Set\", True)\n",
        "  perform_wilcoxon_test(hv_val_unf, hv_val_filt, \"Hypervolume Values on Validation Set\")\n",
        "  avg_HV_val_unf = np.mean(hv_val_unf)\n",
        "  avg_HV_val_filt = np.mean(hv_val_filt)\n",
        "  perform_wilcoxon_test(hv_val_unf, hv_val_filt, \"Average Hypervolume on Validation Set\")\n",
        "\n",
        "  print(\"#\"*30)\n",
        "\n",
        "  perform_wilcoxon_test(numfeatures_minhamloss_test_unf, numfeatures_minhamloss_test_filt, \"Number of Features for Min Ham Loss on Test Set\", True)\n",
        "  perform_wilcoxon_test(minhamloss_test_unf, minhamloss_test_filt, \"Minimum Hamming Loss Values on Test Set\", True)\n",
        "  avg_min_HL_test_unf = np.mean(minhamloss_test_unf)\n",
        "  avg_min_HL_test_filt = np.mean(minhamloss_test_filt)\n",
        "  perform_wilcoxon_test(minhamloss_test_unf, minhamloss_test_filt, \"Average Minimum Hamming Loss on Test Set\", True)\n",
        "  perform_wilcoxon_test(hv_test_unf, hv_test_filt, \"Hypervolume Values on Test Set\")\n",
        "  avg_HV_test_unf = np.mean(hv_test_unf)\n",
        "  avg_HV_test_filt = np.mean(hv_test_filt)\n",
        "  perform_wilcoxon_test(hv_test_unf, hv_test_filt, \"Average Hypervolume on Test Set\")\n",
        "\n",
        "\n",
        "  # Plot for Validation Set\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.suptitle(f'{name}')  # Add a super title for the entire figure\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_features_vs_hl(numfeatures_minhamloss_val_unf, minhamloss_val_unf, 'Validation Set', 'blue', 'Unfiltered')\n",
        "  plot_features_vs_hl(numfeatures_minhamloss_val_filt, minhamloss_val_filt, 'Validation Set', 'red', 'Filtered')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot for Test Set\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_features_vs_hl(numfeatures_minhamloss_test_unf, minhamloss_test_unf, 'Test Set', 'blue', 'Unfiltered')\n",
        "  plot_features_vs_hl(numfeatures_minhamloss_test_filt, minhamloss_test_filt, 'Test Set', 'red', 'Filtered')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JbbEIdi52AFH",
        "outputId": "a4bc0a49-87cc-415b-e173-d51b38286075"
      },
      "outputs": [],
      "source": [
        "# Example use of the read_and_test function. Ensure that the name of the dataset is at the same index as the csv file for the corresponding dataset.\n",
        "dfs = ['Data collection - Enron.csv','Data collection - Birds.csv','Data collection - Coffee.csv','Data collection - Emotions.csv','Data collection - Enron.csv','Data collection - Scene.csv']\n",
        "datasets = ['Enron', 'Birds', 'Coffee', 'Emotions', 'Enron', 'Scene']\n",
        "\n",
        "for df, dats in zip(dfs, datasets):\n",
        "  read_and_test((pd.read_csv(df)).iloc[:-1], dats)\n",
        "  print(\"\\n\")\n",
        "  print(\"#\"*30)\n",
        "  print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
