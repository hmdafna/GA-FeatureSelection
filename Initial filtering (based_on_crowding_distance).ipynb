{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4NfUsqhO0m_"
      },
      "source": [
        "The crowding distance is used to handle the case where all features end up in the top rank. After calculating MIC scores to evaluate feature relevance, the crowding distance is computed to differentiate between features. This distance measures the relative density of features in the rank, helping to select the top 50% of features with the lowest crowding distances. By doing this, the code identifies which features are more distinct within the top rank, allowing for more refined feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o_IoGTQCdEvZ",
        "outputId": "754a6c20-498a-41ca-fa71-ae49e59eeb23"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy scikit-multilearn scipy pymoo matplotlib scikit-learn minepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hIaEbBIdFUU"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from scipy.sparse import lil_matrix\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.config import Config\n",
        "Config.warnings['not_compiled'] = False\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from pymoo.util.nds.non_dominated_sorting import NonDominatedSorting\n",
        "from minepy import pstats, cstats\n",
        "from pymoo.optimize import minimize\n",
        "from multiprocessing import Pool\n",
        "from minepy import MINE  # Make sure to install minepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UfQMWAVysfd"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Explanation\n",
        "find_all_zero_columns: Identifies columns that contain only zeros.\n",
        "calculate_mic: Computes the MIC scores for features.\n",
        "calculate_crowding_distance: Computes crowding distances for features based on their MIC scores.\n",
        "determine_ranks: Integrates the feature selection process by selecting the top 50% of features with the lowest crowding distances.\n",
        "\"\"\"\n",
        "\n",
        "def find_all_zero_columns(X):\n",
        "    zero_columns = X.columns[(X == 0).all()]\n",
        "    zero_column_indices = [X.columns.get_loc(col) for col in zero_columns]\n",
        "    return zero_column_indices\n",
        "\n",
        "def calculate_mic(xtrain_validate, ytrain_validate):\n",
        "    mine = MINE(alpha=9, c=5, est=\"mic_e\")\n",
        "    mic_matrix = []\n",
        "    for feature in xtrain_validate.columns:\n",
        "        mine.compute_score(xtrain_validate[feature], ytrain_validate.iloc[:, 0])\n",
        "        mic_matrix.append(mine.mic())\n",
        "    return np.array(mic_matrix)\n",
        "\n",
        "def calculate_crowding_distance(mic_scores):\n",
        "    num_features = mic_scores.shape[0]\n",
        "\n",
        "    crowding_distance = np.zeros(num_features)\n",
        "\n",
        "    sorted_indices = np.argsort(mic_scores)\n",
        "    sorted_values = mic_scores[sorted_indices]\n",
        "\n",
        "    crowding_distance[sorted_indices[0]] = float('inf')\n",
        "    crowding_distance[sorted_indices[-1]] = float('inf')\n",
        "\n",
        "    for j in range(1, num_features - 1):\n",
        "        crowding_distance[sorted_indices[j]] += (sorted_values[j + 1] - sorted_values[j - 1]) / (sorted_values[-1] - sorted_values[0])\n",
        "\n",
        "    return crowding_distance\n",
        "\n",
        "def determine_ranks(all_Data):\n",
        "    sorted_data = []\n",
        "\n",
        "    for i in range(len(all_Data)):\n",
        "        name, data_matrix, feature_names, labels = all_Data[i]\n",
        "\n",
        "        print(\"The name of the data set is: \", name)\n",
        "        print(\"Feature names:\", feature_names)\n",
        "        print(\"Categories:\", labels)\n",
        "        print(\"Data matrix shape:\", data_matrix.shape)\n",
        "\n",
        "        df = pd.DataFrame(data_matrix, columns=feature_names + labels)\n",
        "\n",
        "        y = df[labels]\n",
        "        X = df.drop(columns=labels)\n",
        "\n",
        "        xtrain_validate, xtest, ytrain_validate, ytest = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "        xtrain, xvalidate, ytrain, yvalidate = train_test_split(xtrain_validate, ytrain_validate, test_size=0.2, random_state=42)\n",
        "\n",
        "        zero_column_indices = find_all_zero_columns(xtrain_validate)\n",
        "        print(\"Indices of columns that are all zeros:\", zero_column_indices)\n",
        "        print(f\"There are {len(zero_column_indices)} features with no presence in the train validate sets\")\n",
        "        print(f\"The shape of this validate data matrix is: {xtrain_validate.shape}\")\n",
        "\n",
        "        mic_scores = calculate_mic(xtrain_validate, ytrain_validate)\n",
        "        crowding_distances = calculate_crowding_distance(mic_scores)\n",
        "\n",
        "        # Select 50% of the features with the lowest crowding distances\n",
        "        num_selected_features = max(1, int(len(crowding_distances) * 0.5))\n",
        "        selected_features_indices = np.argsort(crowding_distances)[:num_selected_features]\n",
        "\n",
        "        print(f\"Selected {num_selected_features} features out of {len(crowding_distances)} based on crowding distance.\")\n",
        "        print(f\"The indices of selected features are: {selected_features_indices}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "        sorted_data.append((name, selected_features_indices))\n",
        "\n",
        "    return sorted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W75kbc8Pdjle"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "all_Data = []\n",
        "\n",
        "# replace 'name' with the dataset's name\n",
        "data_matrix_name, feature_names_name, labels_name = parse_arff_data_name('name.arff')\n",
        "name = ['name', data_matrix_name, feature_names_name, labels_name]\n",
        "all_Data.append(name)\n",
        "\n",
        "sorted_data = determine_ranks(all_Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrg5dHtdzLR2"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import pickle\n",
        "from multiprocessing import Manager\n",
        "from google.colab import files\n",
        "\n",
        "def process_dataset(dataset):\n",
        "    # Extract dataset information\n",
        "    name, data_matrix, feature_names, labels = dataset\n",
        "\n",
        "    # Print dataset being processed\n",
        "    print(f\"Processing dataset: {name}\")\n",
        "\n",
        "    # Call determine_ranks function for the current dataset\n",
        "    sorted_data = determine_ranks([dataset])\n",
        "\n",
        "    return (name, sorted_data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Use multiprocessing.Pool to parallelize processing\n",
        "    with multiprocessing.Pool() as pool:\n",
        "        sorted_datasets = pool.map(process_dataset, all_Data)\n",
        "\n",
        "    # Collect results from multiprocessing.Pool\n",
        "    sorted_data = []\n",
        "    for dataset_result in sorted_datasets:\n",
        "        sorted_data.append(dataset_result)\n",
        "\n",
        "    # Save sorted_data to a pickle file\n",
        "    with open('sorted_data.pkl', 'wb') as f:\n",
        "        pickle.dump(sorted_data, f)\n",
        "\n",
        "    # Download the saved pickle file (Google Colab specific)\n",
        "    files.download('sorted_data.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
